{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "from permanents import glynn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "n_sessions = 100 # number of sessions per iteration\n",
    "Learning_rate = 0.001 # learning rate, increase this to converge faster\n",
    "percentile = 90 # top 100-x percentile the agent will learn from\n",
    "super_percentile = 95 # top 100-x percentile of that survives to the next generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_board(word, n):\n",
    "    word = word.astype(np.float32)  # cast input array to float32\n",
    "    board = np.zeros((n, n), dtype=np.float32)\n",
    "    for i in range(len(word)):\n",
    "        board[i//n, i%n] = word[i]\n",
    "    return board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a helper function that takes in a game and outputs the final board state\n",
    "def final_board_state(game):\n",
    "    n = len(game)\n",
    "    for i in range(len(game)):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        if i == n-1:\n",
    "            return game[i]\n",
    "        if game[i+1].sum() == 0 and game[i].sum() != 0:\n",
    "            return game[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_point_allowed(one_indices, new_point_index, n):\n",
    "    row = new_point_index//n\n",
    "    col = new_point_index%n\n",
    "    point_allowed = True\n",
    "\n",
    "    for i in range(len(one_indices)):\n",
    "        for j in range(i+1, len(one_indices)):\n",
    "            point_one_row = one_indices[i] // n\n",
    "            point_one_col = one_indices[i] % n\n",
    "            point_two_row = one_indices[j] // n\n",
    "            point_two_col = one_indices[j] % n\n",
    "\n",
    "            if (   row == point_one_row\n",
    "                or row == point_two_row\n",
    "                or point_one_row == point_two_row\n",
    "                or col == point_one_col\n",
    "                or col == point_two_col\n",
    "                or point_one_col == point_two_col\n",
    "            ):\n",
    "                continue\n",
    "            \n",
    "            # ensure point_one_col < point_two_col\n",
    "            if point_two_col < point_one_col:\n",
    "                point_one_row, point_two_row = point_two_row, point_one_row\n",
    "                point_one_col, point_two_col = point_two_col, point_one_col\n",
    "\n",
    "            # new point as 1 in a valid 312-pattern\n",
    "            if (point_two_row < point_one_row < row and point_one_col < col < point_two_col):\n",
    "                point_allowed = False\n",
    "                break\n",
    "\n",
    "            # new point as 2 in a valid 312-pattern\n",
    "            if (point_two_row < row < point_one_row and col < point_one_col):\n",
    "                point_allowed = False\n",
    "                break\n",
    "\n",
    "            # new point as 3 in a valid 312-pattern\n",
    "            if (row < point_one_row < point_two_row and point_two_col < col):\n",
    "                point_allowed = False\n",
    "                break\n",
    "\n",
    "        if point_allowed == False:\n",
    "            break\n",
    "\n",
    "    return point_allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines a helper function to add point, action_vec is the output, output = agent(cur_state), agent = n\n",
    "def add_point(input_state, action_vec, n):\n",
    "\n",
    "    point_added = False\n",
    "    action_taken = torch.zeros([len(action_vec)])\n",
    "    cur_state = torch.clone(input_state)\n",
    "\n",
    "    while not point_added:\n",
    "        action_index = torch.multinomial(action_vec, 1).item()\n",
    "        \n",
    "        one_indices = torch.flatten(torch.nonzero(cur_state))\n",
    "        #print(one_indices)\n",
    "        action_allowed = new_point_allowed(one_indices, action_index, n)\n",
    "\n",
    "        if cur_state[action_index] == 0 and action_allowed:\n",
    "            cur_state[action_index] = 1\n",
    "            action_taken[action_index] = 1\n",
    "            point_added = True\n",
    "        else:\n",
    "            action_vec[action_index] = 0\n",
    "            action_vec = action_vec / torch.sum(action_vec)\n",
    "\n",
    "    return cur_state, action_taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(agent, n_sessions, n):\n",
    "    # (nth session, always 4*n - 4 steps, always n*n board)\n",
    "    states = torch.zeros((n_sessions, 4*n-4+1, n*n))\n",
    "    actions = torch.zeros((n_sessions, 4*n-4+1, n*n))\n",
    "    scores = torch.zeros([n_sessions])\n",
    "\n",
    "    states.to(device)\n",
    "\n",
    "    for i in range(n_sessions):\n",
    "        step = 0\n",
    "\n",
    "        while step < 4*n - 4:\n",
    "            step+=1\n",
    "            cur_state = states[i,step-1, :]\n",
    "\n",
    "            output = agent(cur_state)\n",
    "\n",
    "            new_state, action = add_point(cur_state, output, n)\n",
    "\n",
    "            actions[i,step-1, :] = action\n",
    "            states[i,step, :] = new_state\n",
    "\n",
    "        final_state = states[i,step, :]\n",
    "        state_mtx = final_state.reshape(n,n)\n",
    "        scores[i] = glynn(state_mtx.numpy())\n",
    "\n",
    "    return states, actions, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_super_sessions(states_batch, actions_batch, rewards_batch, percentile=90):\n",
    "\n",
    "    counter = n_sessions * (100 - percentile)/100\n",
    "    reward_threshold = np.percentile(rewards_batch, percentile)\n",
    "\n",
    "    super_states = torch.empty(0)\n",
    "    super_actions = torch.empty(0)\n",
    "    super_rewards = torch.empty(0)\n",
    "\n",
    "    for i in range(len(states_batch)):\n",
    "\n",
    "        if counter <= 0:\n",
    "            break\n",
    "\n",
    "        if rewards_batch[i] >= reward_threshold - 0.001:\n",
    "            super_states = torch.cat((super_states, states_batch[i].unsqueeze(0)), dim=0)\n",
    "            super_actions = torch.cat((super_actions, actions_batch[i].unsqueeze(0)), dim=0)\n",
    "            super_rewards = torch.cat((super_rewards, torch.tensor([rewards_batch[i]])), dim=0)\n",
    "            counter -= 1\n",
    "\n",
    "    return super_states, super_actions, super_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_elites(states_batch, actions_batch, rewards_batch, percentile=50):\n",
    "\n",
    "    counter = n_sessions * (100 - percentile)/100\n",
    "    reward_threshold = np.percentile(rewards_batch, percentile)\n",
    "\n",
    "    elite_states = torch.empty(0)\n",
    "    elite_actions = torch.empty(0)\n",
    "\n",
    "    for i in range(len(states_batch)):\n",
    "\n",
    "        if counter <= 0:\n",
    "            break\n",
    "\n",
    "        if rewards_batch[i] >= reward_threshold - 0.01:\n",
    "            game_end_index = 0\n",
    "            for item in states_batch[i]:\n",
    "                if item.sum() == 0 and game_end_index != 0:\n",
    "                    break\n",
    "                elite_states = torch.cat((elite_states, item.unsqueeze(0)))\n",
    "                game_end_index += 1\n",
    "\n",
    "            for item in actions_batch[i]:\n",
    "                if game_end_index == 0:\n",
    "                    break\n",
    "                elite_actions = torch.cat((elite_actions, item.unsqueeze(0)))\n",
    "                game_end_index -= 1\n",
    "            counter -= 1\n",
    "\n",
    "    return elite_states, elite_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(board_size, filename):\n",
    "    n_sessions = 100 # number of sessions per iteration\n",
    "    Learning_rate = 0.0001 # learning rate, increase this to converge faster\n",
    "\n",
    "    n = board_size\n",
    "\n",
    "    input_space = n*n\n",
    "    INF = 1000000\n",
    "\n",
    "    first_layer_neurons = 128\n",
    "    second_layer_neurons = 64\n",
    "    third_layer_neurons = 4\n",
    "    last_layer_neurons = n*n\n",
    "\n",
    "    # Defining the neural network architecture\n",
    "    class MyNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(MyNet, self).__init__()\n",
    "            self.fc1 = nn.Linear(input_space, first_layer_neurons)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.fc2 = nn.Linear(first_layer_neurons, second_layer_neurons)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.fc3 = nn.Linear(second_layer_neurons, third_layer_neurons)\n",
    "            self.relu = nn.ReLU()\n",
    "            self.fc4 = nn.Linear(third_layer_neurons, last_layer_neurons)\n",
    "            self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.relu(self.fc1(x))\n",
    "            x = self.relu(self.fc2(x))\n",
    "            x = self.relu(self.fc3(x))\n",
    "            x = self.softmax(self.fc4(x))\n",
    "\n",
    "            return x\n",
    "\n",
    "    # Create neural network for all other points\n",
    "    net = MyNet()\n",
    "\n",
    "    # Definte the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=Learning_rate)\n",
    "\n",
    "    global super_states\n",
    "    super_states = torch.empty((0, n*n, n*n), dtype=torch.int)\n",
    "    global super_actions\n",
    "    super_actions = torch.tensor([], dtype=torch.int)\n",
    "    global super_rewards\n",
    "    super_rewards = torch.tensor([])\n",
    "\n",
    "    counter = 0\n",
    "    pass_threshold = 1.25 * n\n",
    "\n",
    "    cur_best_reward = 0\n",
    "    cur_best_board = torch.zeros([n*n])\n",
    "    cur_best_game = torch.zeros([n*n, n*n])\n",
    "    cur_best_actions = torch.zeros([n*n, n*n])\n",
    "\n",
    "    for i in range(10):\n",
    "        states_batch, actions_batch, rewards_batch = generate_session(net, n_sessions, n)\n",
    "\n",
    "        #states_batch = states_batch.to(dtype=torch.to)\n",
    "\n",
    "        if i > 0:\n",
    "            states_batch = torch.cat((states_batch, super_states), dim=0)\n",
    "            actions_batch = torch.cat((actions_batch, super_actions), dim=0)\n",
    "            rewards_batch = torch.cat((rewards_batch, super_rewards), dim=0)\n",
    "\n",
    "        elite_states, elite_actions = select_elites(states_batch, actions_batch, rewards_batch, percentile = percentile)\n",
    "\n",
    "        super_sessions = select_super_sessions(states_batch, actions_batch, rewards_batch, percentile = super_percentile)\n",
    "\n",
    "\n",
    "        super_sessions = [(super_sessions[0][i], super_sessions[1][i], super_sessions[2][i]) for i in range(len(super_sessions[2]))] #, super_sessions[3][i]\n",
    "        super_sessions.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        elite_states.to(device)\n",
    "        outputs = net(elite_states)\n",
    "\n",
    "        loss = criterion(outputs, elite_actions.float())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        super_states = torch.stack([super_sessions[i][0] for i in range(len(super_sessions))])\n",
    "        super_actions = torch.stack([super_sessions[i][1] for i in range(len(super_sessions))])\n",
    "        super_rewards = torch.stack([super_sessions[i][2] for i in range(len(super_sessions))])\n",
    "\n",
    "        ########\n",
    "\n",
    "        mean_all_reward = torch.mean(rewards_batch[-100:])\n",
    "        mean_best_reward = torch.mean(super_rewards)\n",
    "\n",
    "        #acceptence_threshold = mean_all_reward - 1\n",
    "        #explore_rate = 1-(i/10000)\n",
    "\n",
    "        #if mean_best_reward > 1.25*n:\n",
    "        #    counter+=1\n",
    "\n",
    "        print(\"\\n\" + str(i) +  \". Best individuals: \" + str(super_rewards))#str(np.flip(np.sort(super_rewards))))\n",
    "\n",
    "        #uncomment below line to print out how much time each step in this loop takes.\n",
    "        #print(\t\"Mean reward: \" + str(mean_all_reward) + \"\\nSessgen: \" + str(sessgen_time) + \", other: \" + str(randomcomp_time) + \", select1: \" + str(select1_time) + \", select2: \" + str(select2_time) + \", select3: \" + str(select3_time) +  \", fit: \" + str(fit_time) + \", score: \" + str(score_time))\n",
    "\n",
    "        #uncomment below line to print out the mean best reward\n",
    "        print(\"Mean best reward: \" + str(mean_best_reward))\n",
    "        #print(\"Best reward: \" + str(np.flip(np.sort(super_rewards))[0]))\n",
    "    \n",
    "        # Make a new folder if 'Data' folder does not exist\n",
    "        #if not os.path.exists('Data'):\n",
    "        #    os.makedirs('Data')\n",
    "\n",
    "        #max_index = torch.argmax(super_rewards)\n",
    "        max_index = 0\n",
    "        \n",
    "        #print('old best: ' + str(cur_best_reward))\n",
    "        #print('test super rewards max index: ' + str(super_rewards[max_index]))\n",
    "        if super_rewards[max_index] > cur_best_reward:\n",
    "            cur_best_reward = super_rewards[max_index]\n",
    "            #print('new best: ' + str(cur_best_reward))\n",
    "            #cur_best_board = final_board_state(super_states[max_index]).numpy()\n",
    "            cur_best_board = super_states[max_index, 4*n-4].numpy()\n",
    "            cur_best_game = super_states[max_index]\n",
    "            cur_best_actions = super_actions[max_index]\n",
    "\n",
    "            best_states_set = set()\n",
    "            best_states_set.add(str(cur_best_board))\n",
    "\n",
    "            with open(os.path.join('Data', str(filename)+'_best_board_timeline'+'.txt'), 'a') as f:\n",
    "                f.write(str(convert_to_board(cur_best_board, n))) #, construction)))\n",
    "                f.write(\"\\n\")\n",
    "            with open(os.path.join('Data', str(filename)+'_best_reward_timeline'+'.txt'), 'a') as f:\n",
    "                f.write(str(cur_best_reward))\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "            #with open(os.path.join('Data', str(filename)+'_best_board_timeline'+'.txt'), 'a') as f:\n",
    "            #    f.write(str(convert_to_board(cur_best_board, n))) #, construction)))\n",
    "            #    f.write(\"\\n\")\n",
    "            \n",
    "            #with open(os.path.join('Data', str(filename)+'_best_reward_timeline'+'.txt'), 'a') as f:\n",
    "            #    f.write(str(cur_best_reward))\n",
    "            #    f.write(\"\\n\")\n",
    "            \n",
    "        #    counter = 0\n",
    "        \n",
    "        if super_rewards[max_index] == cur_best_reward:\n",
    "        #    counter += 1\n",
    "\n",
    "            cur_best_board = super_states[max_index, 4*n-4].numpy()\n",
    "            if str(cur_best_board) not in best_states_set:\n",
    "                best_states_set.add(str(cur_best_board))\n",
    "                #print('Glynn: '+str(glynn(cur_best_board.reshape(n,n))))\n",
    "                #print(str(cur_best_board))\n",
    "                with open(os.path.join('Data', str(filename)+'_best_board_timeline'+'.txt'), 'a') as f:\n",
    "                    f.write(str(convert_to_board(cur_best_board, n))) #, construction)))\n",
    "                    f.write(\"\\n\")\n",
    "                with open(os.path.join('Data', str(filename)+'_best_reward_timeline'+'.txt'), 'a') as f:\n",
    "                    f.write(str(cur_best_reward))\n",
    "                    f.write(\"\\n\")\n",
    "                \n",
    "        #   cur_best_board = final_board_state(super_states[max_index]).numpy()\n",
    "            #if str(cur_best_board) not in best_states_set:\n",
    "            #    with open(os.path.join('Data', str(filename)+'_best_board_timeline'+'.txt'), 'a') as f:\n",
    "            #        f.write(str(convert_to_board(cur_best_board, n))) #, construction)))\n",
    "            #        f.write(\"\\n\")\n",
    "            #    with open(os.path.join('Data', str(filename)+'_best_reward_timeline'+'.txt'), 'a') as f:\n",
    "            #        f.write(str(cur_best_reward))\n",
    "            #        f.write(\"\\n\")\n",
    "        \n",
    "        #if board_type == \"line\" and cur_best_reward == 4*n:\n",
    "        #    return net\n",
    "        \n",
    "        #if counter > 1000:\n",
    "        #    return net\n",
    "        '''\n",
    "        if write_all:\n",
    "            if (i%20 == 1): #Write all important info to files every 20 iterations\n",
    "                with open(os.path.join('Data', str(filename)+'_best_species'+'.txt'), 'w') as f:\n",
    "                    for game in super_states:\n",
    "                        f.write(str(convert_to_board(final_board_state(game).numpy(), n))) #, construction)))\n",
    "                        f.write(\"\\n\")\n",
    "                with open(os.path.join('Data', str(filename)+'_best_species_rewards'+'.txt'), 'w') as f:\n",
    "                    for item in super_rewards:\n",
    "                        f.write(str(item))\n",
    "                        f.write(\"\\n\")\n",
    "                with open(os.path.join('Data', str(filename)+'_best_100_rewards'+'.txt'), 'a') as f:\n",
    "                    f.write(str(mean_all_reward)+\"\\n\")\n",
    "                with open(os.path.join('Data', str(filename)+'_best_super_rewards'+'.txt'), 'a') as f:\n",
    "                    f.write(str(mean_best_reward)+\"\\n\")\n",
    "                if (i%200==2):\n",
    "                    with open(os.path.join('Data', str(filename)+'_best_species_timeline'+'.txt'), 'a') as f:\n",
    "                        f.write(str(convert_to_board(final_board_state(super_states[max_index]).numpy(), n))) #, construction)))\n",
    "                        f.write(\"\\n\")\n",
    "        if write_best:\n",
    "            if mean_best_reward > pass_threshold:\n",
    "                with open(os.path.join('Data', str(filename)+'_best_species'+'.txt'), 'w') as f:\n",
    "                    for game in super_states:\n",
    "                        f.write(str(convert_to_board(final_board_state(game).numpy(), n))) #,construction)))\n",
    "                        f.write(\"\\n\")\n",
    "                with open(os.path.join('Data', str(filename)+'_best_species_rewards'+'.txt'), 'w') as f:\n",
    "                    for item in super_rewards:\n",
    "                        f.write(str(item))\n",
    "                        f.write(\"\\n\")\n",
    "                with open(os.path.join('Data', str(filename)+'_best_100_rewards'+'.txt'), 'a') as f:\n",
    "                    f.write(str(mean_all_reward)+\"\\n\")\n",
    "                with open(os.path.join('Data', str(filename)+'_best_super_rewards'+'.txt'), 'a') as f:\n",
    "                    f.write(str(mean_best_reward)+\"\\n\")\n",
    "                if (i%200==2):\n",
    "                    max_index = torch.argmax(super_rewards)\n",
    "                    with open(os.path.join('Data', str(filename)+'_best_species_timeline'+'.txt'), 'a') as f:\n",
    "                        f.write(str(convert_to_board(final_board_state(super_states[max_index]).numpy(), n))) #, construction)))\n",
    "                        f.write(\"\\n\")\n",
    "\n",
    "        if counter > 1000:\n",
    "            if mean_best_reward > pass_threshold:\n",
    "                with open(os.path.join('Data', str(filename)+'_best_species'+'.txt'), 'w') as f:\n",
    "                    for game in super_states:\n",
    "                        f.write(str(convert_to_board(final_board_state(game).numpy(), n))) #, construction)))\n",
    "                        f.write(\"\\n\")\n",
    "                with open(os.path.join('Data', str(filename)+'_best_species_rewards'+'.txt'), 'w') as f:\n",
    "                    for item in super_rewards:\n",
    "                        f.write(str(item))\n",
    "                        f.write(\"\\n\")\n",
    "                with open(os.path.join('Data', str(filename)+'_best_100_rewards'+'.txt'), 'a') as f:\n",
    "                    f.write(str(mean_all_reward)+\"\\n\")\n",
    "                with open(os.path.join('Data', str(filename)+'_best_super_rewards'+'.txt'), 'a') as f:\n",
    "                    f.write(str(mean_best_reward)+\"\\n\")\n",
    "                if (i%200==2):\n",
    "                    max_index = torch.argmax(super_rewards)\n",
    "                    with open(os.path.join('Data', str(filename)+'_best_species_timeline'+'.txt'), 'a') as f:\n",
    "                        f.write(str(convert_to_board(final_board_state(super_states[max_index]).numpy(), n))) #, construction)))\n",
    "                        f.write(\"\\n\n",
    "        #        return net\n",
    "        '''\n",
    "    print(cur_best_reward)\n",
    "    return net, cur_best_game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0. Best individuals: tensor([25., 23., 22., 22., 22.])\n",
      "Mean best reward: tensor(22.8000)\n",
      "\n",
      "1. Best individuals: tensor([28., 24., 24., 24., 24.])\n",
      "Mean best reward: tensor(24.8000)\n",
      "\n",
      "2. Best individuals: tensor([28., 25., 24., 24., 24.])\n",
      "Mean best reward: tensor(25.)\n",
      "\n",
      "3. Best individuals: tensor([32., 28., 28., 25., 25.])\n",
      "Mean best reward: tensor(27.6000)\n",
      "\n",
      "4. Best individuals: tensor([32., 28., 28., 27., 25.])\n",
      "Mean best reward: tensor(28.)\n",
      "\n",
      "5. Best individuals: tensor([32., 28., 28., 26., 25.])\n",
      "Mean best reward: tensor(27.8000)\n",
      "\n",
      "6. Best individuals: tensor([32., 26., 25., 25., 25.])\n",
      "Mean best reward: tensor(26.6000)\n",
      "\n",
      "7. Best individuals: tensor([32., 26., 26., 25., 25.])\n",
      "Mean best reward: tensor(26.8000)\n",
      "\n",
      "8. Best individuals: tensor([32., 26., 26., 26., 26.])\n",
      "Mean best reward: tensor(27.2000)\n",
      "\n",
      "9. Best individuals: tensor([32., 26., 26., 25., 25.])\n",
      "Mean best reward: tensor(26.8000)\n",
      "tensor(32.)\n"
     ]
    }
   ],
   "source": [
    "n = 6\n",
    "filename = \"6x6_test\"\n",
    "best_net, best_game = train(n, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 943,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_all = False\n",
    "write_best = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previous point insertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to add 3 of the 5 free points present in all 312-avoiding matrices\n",
    "def add_free_points(input_state, n, step):\n",
    "    action_taken = torch.zeros([len(input_state)])\n",
    "    cur_state = torch.clone(input_state)\n",
    "\n",
    "    if step == 1:\n",
    "        action_index = 0\n",
    "    elif step == 2:\n",
    "        action_index = n**2-n\n",
    "    else: # step == 3:\n",
    "        action_index = n**2-1\n",
    "\n",
    "    cur_state[action_index] = 1\n",
    "    action_taken[action_index] = 1\n",
    "\n",
    "    return cur_state, action_taken    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to add corners based on probability on the output of the corner_agent neural network\n",
    "def add_corner(input_state, action_vec, n, row_boundary, col_boundary):\n",
    "    corner_added = False\n",
    "    action_taken = torch.zeros([len(action_vec)])\n",
    "    cur_state = torch.clone(input_state)\n",
    "\n",
    "    terminal = False\n",
    "\n",
    "    while not corner_added:\n",
    "        action_index = torch.multinomial(action_vec, 1).item()\n",
    "        action_row = action_index//n\n",
    "        action_col = action_index%n\n",
    "\n",
    "        if (action_row == n-2 and action_col != n-1) or (action_col == 1 and action_row != 0) or (row_boundary == 0 and action_row != 0):\n",
    "            action_vec[action_index] = 0\n",
    "            action_vec = action_vec / torch.sum(action_vec)\n",
    "        elif row_boundary <= action_row < n-1 and col_boundary <= action_col:\n",
    "            cur_state[action_index] = 1\n",
    "            action_taken[action_index] = 1\n",
    "            corner_added = True\n",
    "        else:\n",
    "            action_vec[action_index] = 0\n",
    "            action_vec = action_vec / torch.sum(action_vec)\n",
    "\n",
    "    if action_col == n-1:\n",
    "        terminal = True\n",
    "\n",
    "    return cur_state, action_taken, terminal, action_row, action_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines a helper function to add point, action_vec is the output, output = agent(cur_state), agent = net\n",
    "def add_point(input_state, action_vec, forbidden_state, corners, n):\n",
    "    \n",
    "    ## add time\n",
    "    point_added = False\n",
    "    action_taken = torch.zeros([len(action_vec)])\n",
    "    cur_state = torch.clone(input_state)\n",
    "    cur_forbidden = torch.clone(forbidden_state)\n",
    "\n",
    "    while not point_added:\n",
    "        action_index = torch.multinomial(action_vec, 1).item()\n",
    "\n",
    "        if cur_state[action_index] == 0 and cur_forbidden[action_index] != 1:\n",
    "            # action\n",
    "            cur_state[action_index] = 1\n",
    "            action_taken[action_index] = 1\n",
    "            point_added = True\n",
    "\n",
    "            point_row = action_index//n\n",
    "            point_col = action_index%n\n",
    "            # fill forbidden\n",
    "            for corner in corners:\n",
    "                corner_row = corner//n\n",
    "                corner_col = corner%n\n",
    "                # fill left block\n",
    "                if corner_row < point_row and point_col < corner_col:\n",
    "                    for forbidden_row in range(corner_row+1, point_row):\n",
    "                        for forbidden_col in range(point_col):\n",
    "                            forbidden_index = forbidden_row*n + forbidden_col\n",
    "                            if cur_state[forbidden_index] == 0:\n",
    "                                cur_forbidden[forbidden_index] = 1\n",
    "                    # fill right block\n",
    "                    for forbidden_col in range(point_col+1, corner_col):\n",
    "                        for forbidden_row in range(point_row+1, n):\n",
    "                            forbidden_index = forbidden_row*n + forbidden_col\n",
    "                            if cur_state[forbidden_index] == 0:\n",
    "                                cur_forbidden[forbidden_index] = 1\n",
    "        else:\n",
    "            action_vec[action_index] = 0\n",
    "            action_vec = action_vec / torch.sum(action_vec)\n",
    "\n",
    "    return cur_state, action_taken, cur_forbidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, scores = generate_session(best_net, best_corner_net, 1, 5) #  forbidden_points,\n",
    "\n",
    "# Plot\n",
    "step_through = 4*n - 4\n",
    "state = states[0,step_through,:]\n",
    "state_mtx = state.reshape(n,n)\n",
    "\n",
    "action = actions[0,step_through-1,:]\n",
    "action_mtx = action.reshape(n,n)\n",
    "\n",
    "#forbidden = forbidden_points[0,step_through,:]\n",
    "#forbidden_mtx = forbidden.reshape(n,n)\n",
    "\n",
    "plot_board(state_mtx)\n",
    "#plot_board(action_mtx)\n",
    "#plot_board(forbidden_mtx)\n",
    "\n",
    "print(glynn(state_mtx.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "state = best_game[4*n-4,:]\n",
    "state_mtx = state.reshape(n,n)\n",
    "plot_board(state_mtx)\n",
    "print(glynn(state_mtx.numpy()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
